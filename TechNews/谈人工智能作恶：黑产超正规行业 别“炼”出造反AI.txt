           谈人工智能作恶：黑产超正规行业 别“炼”出造反AI


　　谈人工智能作恶：黑产超正规行业 别“炼”出造反AI
　　直面隐忧 业界大咖谈人工智能作恶
　　别“炼”出造反的AI
　　来源：科技日报
　　本报记者 刘 垠
　　一场抢劫案后，格雷的妻子丧生，自己也全身瘫痪。他接受了一个天才科学家的“升级”改造治疗——在他身体里植入了人工智能程序STEM，获得了超强的能力，从一个“残废”直接升级成为职业杀手。随着STEM的进化升级，步步紧逼格雷交出身体使用权和大脑意识控制权……
　　本年度关于人工智能和人类未来的最佳影片，不少人认为非《升级》莫属。而人工智能和人类抗衡的探讨，是科幻电影中的永恒话题，从《银翼杀手》到《机械姬》，再到今年的低成本电影《升级》，都映射出未来人工智能对人类的威胁。
　　黑产超正规行业 恶意源于人类基因
　　AI造反，是科幻电影里太常见的桥段。问题在于，现实当中真正的AI好像也在一步步向我们走来。不少人抱有忧虑和不安，人工智能会“作恶”吗？
　　倾向于AI威胁论的人并不在少数。马斯克曾在推特上表示：“我们要非常小心人工智能，它可能比核武器更危险。”史蒂芬·霍金也说：“人工智能可能是一个‘真正的危险’。机器人可能会找到改进自己的办法，而这些改进并不总是会造福人类。”
　　“任何技术都是一把双刃剑，都有可能用于作恶，为什么人工智能作恶会引起这么大的反响？”在近日召开的2018中国计算机大会的分论坛上，哈尔滨工业大学长聘教授邬向前抛出了问题，人工智能研究的底线到底在哪里？
　　早在1942年，阿西莫夫就提出了机器人三定律。但问题在于，这些科幻书中美好的定律，执行时会遇到很大的问题。
　　“一台计算机里跑什么样的程序，取决于这个程序是谁写的。”360集团技术总裁、首席安全官谭晓生说，机器人的定律可靠与否，首先是由人定义的，然后由机器去存储、执行。
　　值得注意的是，“不作恶”已成科技行业的一个技术原则。那么，机器人作恶，恶意到底从何而来？
　　如今人工智能发展的如火如荼，最早拥抱AI的却是黑产群体，包括用AI的方法来突破验证码，去黑一些账户。谭晓生笑言：“2016年中国黑产的收入已超过一千亿，整个黑产比我们挣的钱还要多，它怎么会没有动机呢？”
　　“AI作恶的实质，是人类在作恶。”北京大学法学院教授张平认为，AI不过是一个工具，如果有人拿着AI去作恶，那就应该制裁AI背后的人，比如AI的研发人员、控制者、拥有者或是使用者。当AI在出现损害人类、损害公共利益和市场规则的“恶”表现时，法律就要出来规制了。
　　目前，无人驾驶和机器人手术时引发的事故，以及大数据分析时的泛滥和失控时有耳闻。那么，人工智能会进化到人类不可控吗？届时AI作恶，人类还能招架的住吗？
　　任务驱动型AI 还犯不了“反人类罪”
　　值得关注的是，霍金在其最后的著作中向人类发出警告，“人工智能的短期影响取决于谁来控制它，长期影响则取决于它能否被控制。”言下之意，人工智能真正的风险不是恶意，而是能力。
　　“人工智能未来的发展会威胁到人类的生存，这不是杞人忧天，确实会有很大的风险，虽说不是一定会发生，但是有很大的概率会发生。”在谭晓生看来，人类不会被灭亡，不管人工智能如何进化，总会有漏洞，黑客们恰恰会在极端的情况下找到一种方法把这个系统完全摧毁。
　　对此，上海交通大学电子系特别研究员倪冰冰持乐观态度。“我们目前大部分的AI技术是任务驱动型，AI的功能输出、输入都是研究者、工程师事先规定好的。”倪冰冰解释说，绝大多数的AI技术远远不具备反人类的能力，至少目前不用担心。
　　张平表示，当AI发展到强人工智能阶段时，机器自动化的能力提高了，它能够自我学习、自我升级，会拥有很强大的功能。比如人的大脑和计算机无法比拟时，这样的强人工智能就会对我们构成威胁。
　　“人类给AI注入什么样的智慧和价值观至关重要，但若AI达到了人类无法控制的顶级作恶——‘反人类罪’，就要按照现行人类法律进行处理。”张平说，除了法律之外，还需有立即“处死”这类AI的机制，及时制止其对人类造成的更大伤害。“这要求在AI研发中必须考虑‘一键瘫痪’的技术处理，如果这样的技术预设做不到，这类AI就该停止投资与研发，像人类对待毒品般全球诛之。”
　　作恶案底渐增 预防机制要跟上
　　事实上，人们的担忧并非空穴来风。人工智能作恶的事件早在前两年就初见端倪，比如职场偏见、政治操纵、种族歧视等。此前，德国也曾发生人工智能机器人把管理人员杀死在流水线的事件。
　　可以预见，AI作恶的案例会日渐增多，人类又该如何应对？
　　“如果我们把AI当作工具、产品，从法律上来说应该有一种预防的功能。科学家要从道德的约束、技术标准的角度来进行价值观的干预。”张平强调，研发人员不能给AI灌输错误的价值观。毕竟，对于技术的发展，从来都是先发展再有法律约束。
　　在倪冰冰看来，目前不管是AI算法还是技术，都是人类在进行操控，我们总归有一些很强的控制手段，控制AI在最高层次上不会对人产生一些负面影响。“如果没有这样一个操控或者后门的话，那意味着不是AI在作恶，而是发明这个AI工具的人在作恶。”
　　凡是技术，就会有两面性。为什么我们会觉得人工智能的作恶让人更加恐惧？与会专家直言，是因为AI的不可控性，在黑箱的情况下，人对不可控东西的恐惧感更加强烈。
　　目前最火的领域——“深度学习”就是如此，行业者将其戏谑地称为“当代炼金术”，输入各类数据训练AI，“炼”出一堆我们也不知道为啥会成这样的玩意儿。人类能信任自己都无法理解的决策对象吗？
　　显然，技术开发的边界有必要明晰，比尔·盖茨也表示担忧。他认为，现阶段人类除了要进一步发展AI技术，同时也应该开始处理AI造成的风险。然而，“这些人中的大多数都没有研究AI风险，只是在不断加速AI发展。”
　　业界专家呼吁，我们必须清楚地知道人工智能会做出什么样的决策，对人工智能的应用范围和应用结果的预期，一定要有约束。
　　AI会不会进化，未来可能会形成一个AI社会吗？“AI也许会为了争取资源来消灭人类，这完全有可能，所以我们还是要重视AI作恶的程度和风险。”现场一位嘉宾建议，我们现在要根据人工智能的不同阶段，比如弱智能、强智能和超智能，明确哪些人工智能应该研究，哪些应该谨慎研究，而哪些又是绝对不能研究的。
　　如何防范AI在极速前进的道路上跑偏？“要从技术、法律、道德、自律等多方面预防。”张平说，AI研发首先考虑道德约束，在人类不可预见其后果的情况下，研发应当慎重。同时，还需从法律上进行规制，比如联合建立国际秩序，就像原子弹一样，不能任其无限制地发展。







                        (function(){
                            var adScript = document.createElement('script');
                            adScript.src = '//d1.sina.com.cn/litong/zhitou/sinaads/demo/wenjing8/js/yl_left_hzh_20171020.js';
                            document.getElementsByTagName('head')[0].appendChild(adScript);
                        })();
                    


