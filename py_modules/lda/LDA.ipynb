{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_path = \"../../stop_words.txt\"\n",
    "stop_words_f = open(stop_words_path, 'r')\n",
    "stop_words_content = stop_words_f.read()\n",
    "#将停用词表转换为list\n",
    "stop_words_list = stop_words_content.splitlines()\n",
    "stop_words_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, n_words):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        features = \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_words - 1:-1]])\n",
    "        print(\"Topic {}: {}\".format(idx, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "def build_word_bags():\n",
    "    folder = \"../../news/after_process\"\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    \"\"\"\n",
    "    Read all \"sports\" news and use \"jieba\" to segment each article into\n",
    "    a string of words seperated by \"\\\\b\" \n",
    "    \"\"\"\n",
    "    for file in sorted(os.listdir(folder)):\n",
    "        try:\n",
    "            filepath = os.path.join(folder, file)\n",
    "            f = open(filepath, 'r')\n",
    "            article = f.read()\n",
    "            segment_list = jieba.cut(article, cut_all=False)\n",
    "            splitted = \" \".join(segment_list)\n",
    "            corpus.append(splitted)\n",
    "            f.close()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return corpus\n",
    "\n",
    "# def display_topics(model, feature_names, no_top_words):\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "#         print()\"Topic %d:\" % (topic_idx)\n",
    "#         print \" \".join([feature_names[i]\n",
    "#                         for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "n_compnts = 6\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stop_words_list)\n",
    "tf = tf_vectorizer.fit_transform(build_word_bags())\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "alphas = [0.1]\n",
    "etas = [0.5]\n",
    "for alpha, eta in zip(alphas, etas):\n",
    "    print(\"Alpha: {}, Eta: {}\".format(alpha, eta))\n",
    "    try:\n",
    "        lda = LatentDirichletAllocation(n_components=n_compnts, doc_topic_prior=alpha, \n",
    "                                        topic_word_prior=eta, verbose=1, evaluate_every=15,\n",
    "                                        max_iter=90, learning_method='online', learning_offset=2,\n",
    "                                        random_state=0, n_jobs=-1).fit(tf)\n",
    "        picket.dump(\"lda-alpha-{}-eta-{}\".format(alpha, eta), lda)\n",
    "    except:\n",
    "        continue\n",
    "# X = lda.transform(tf)\n",
    "#     print(\"\\n Fit LDA to data set\")\n",
    "# display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
