{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_path = \"../../stop_words.txt\"\n",
    "# stop_words_path = \"../../old_stop_words.txt\"\n",
    "stop_words_f = open(stop_words_path, 'r')\n",
    "stop_words_content = stop_words_f.read()\n",
    "#将停用词表转换为list\n",
    "stop_words_list = stop_words_content.splitlines()\n",
    "stop_words_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../../news/new_process\"\n",
    "corpus = []\n",
    "distribution = [0 for i in range(6)]\n",
    "\"\"\"\n",
    "Read all \"sports\" news and use \"jieba\" to segment each article into\n",
    "a string of words seperated by \"\\\\b\" \n",
    "\"\"\"\n",
    "for file in sorted(os.listdir(folder)):\n",
    "    try:\n",
    "        filepath = os.path.join(folder, file)\n",
    "        f = open(filepath, 'r')\n",
    "        article = f.read()\n",
    "        segment_list = jieba.cut(article, cut_all=False)\n",
    "        splitted = \" \".join(segment_list)\n",
    "        file_class = int(file[0]) - 1\n",
    "        distribution[file_class] += 1\n",
    "        corpus.append(splitted)\n",
    "        f.close()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build TF-IDF using CountVectorizer, splitting words by spaces\n",
    "\"\"\"\n",
    "# step 1\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=1.0, token_pattern='\\\\b\\\\w+\\\\b', stop_words=stop_words_list)\n",
    "# step 2\n",
    "vectorizer.fit(corpus)\n",
    "# step 3\n",
    "X = vectorizer.transform(corpus)\n",
    "print(X.shape)\n",
    "# step 4\n",
    "# tfidf_transformer = TfidfTransformer(norm='l1')\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "# step 5\n",
    "tfidf = tfidf_transformer.fit_transform(X.toarray())\n",
    "print(tfidf.shape)\n",
    "# step 6\n",
    "# for idx, word in enumerate(vectorizer.get_feature_names()):\n",
    "#     print(\"{}\\t{}\".format(word, tfidf_transformer.idf_[idx]))\n",
    "# step 4\n",
    "ndarray = tfidf.toarray()\n",
    "print(ndarray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = [1875, 1498, 1944, 1595, 1786, 1944]\n",
    "for i in range(len(distribution)):\n",
    "    distribution[i] += distribution[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroup(x):\n",
    "    g = 0\n",
    "    for i in range(len(distribution)):\n",
    "        if x > distribution[i]:\n",
    "            g = i\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [6, 10, 15, 20, 25, 30]\n",
    "for n in n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n, random_state=34312, n_init=10, n_jobs=-1).fit(data)\n",
    "    labels = kmeans.labels_\n",
    "    dic = {}\n",
    "    for i in range(n):\n",
    "        dic[i] = {}\n",
    "        for j in range(len(distribution)):\n",
    "            dic[i][j] = 0\n",
    "    for i in range(len(labels)):\n",
    "        g = getGroup(i)\n",
    "        dic[labels[i]][g] += 1\n",
    "    percents = {}\n",
    "    size = {}\n",
    "    for k in dic:\n",
    "        _max = 0\n",
    "        total = 0\n",
    "        for sk in dic[k]:\n",
    "            total += dic[k][sk]\n",
    "            _max = max(_max, dic[k][sk])\n",
    "        if total != 0:\n",
    "            percent = _max / total\n",
    "        else:\n",
    "            percent = -1\n",
    "        percents[k] = percent\n",
    "        size[k] = total\n",
    "    print(\"------Num of Clusters: {}-------\".format(n))\n",
    "    sum_weighted_purity = 0\n",
    "    for k in percents:\n",
    "        sum_weighted_purity += percents[k] * size[k]\n",
    "    weighted_purity = sum_weighted_purity / len(labels)\n",
    "    print(\"------Cluster Distribution------\")\n",
    "    print(size)\n",
    "    print(\"------Weighted Purity: {}-------\".format(weighted_purity))\n",
    "    print(\"------SSE: {}-------\".format(kmeans.inertia_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
